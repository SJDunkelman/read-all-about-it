{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newspaper Sentiment Analysis\n",
    "\n",
    "S. Dunkelman\n",
    "\n",
    "The emotive language used by online news sources within the UK and US was measured during the period from when Covid-19 was first reported (4th January) up until today (13th May). \n",
    "\n",
    "This report uses a dataset of more than ### news stories to measure sentiment across online news sources during the Covis-19 2020 pandemic. A lexicon-based methodology was employed at the phrase level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data period covers 114 days.\n"
     ]
    }
   ],
   "source": [
    "# Import user modules\n",
    "from scraper import Scraper\n",
    "from newspapers import Guardian, NYPost, WSJ, LATimes, NYTimes, MotherJones\n",
    "from sentiment import Sentiment\n",
    "\n",
    "# Define hyperparameters\n",
    "from datetime import date\n",
    "FIRST_DATE = date(2020,1,1)\n",
    "LAST_DATE = date(2020,4,24)\n",
    "TIME_FRAME = (LAST_DATE - FIRST_DATE).days\n",
    "print(\"Data period covers\",TIME_FRAME,\"days.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Extraction\n",
    "We first generate a queue of URLs covering all the published stories before the text data is extracted. Whilst categorisation of articles differs based on the source, the samples from each were restricted to their respective national coverage in order to avoid duplication of stories internationally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping took 590.4554071500006 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Scrape only headlines\n",
    "bot = Scraper(full_text=0)\n",
    "\n",
    "# Add news sources\n",
    "bot.add(Guardian(bot.date,TIME_FRAME))\n",
    "\n",
    "# Time performance\n",
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "\n",
    "# Scrape sources in queue\n",
    "bot.scrape()\n",
    "end = timer()\n",
    "print(\"Scraping took\",round(end-start),\"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyschosocial Categorisation\n",
    "Next the General Inquirer's (GI) Harvard IV-4 Pyschosocial dictionary was used to systematically analyse each text sample by counting words within each category (Tetlock 2007)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
